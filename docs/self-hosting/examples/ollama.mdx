---
title: Integrating OpenGPT with Ollama for Enhanced Language Models
description: >-
  Learn how to configure and deploy OpenGPT to leverage Ollama's powerful
  language models locally. Follow the guide to run Ollama and OpenGPT on your
  system.
tags:
  - Ollama integration
  - OpenGPT configuration
  - Local deployment
  - Language models
  - Ollama usage
---

# Integrating with Ollama

Ollama is a powerful framework for running large language models (LLMs) locally, supporting various language models including Llama 2, Mistral, and more. Now, OpenGPT supports integration with Ollama, meaning you can easily use the language models provided by Ollama to enhance your application within OpenGPT.

This document will guide you on how to configure and deploy OpenGPT to use Ollama:

## Running Ollama Locally

First, you need to install Ollama. For detailed steps on installing and configuring Ollama, please refer to the [Ollama Website](https://ollama.com).

## Running OpenGPT Locally

Assuming you have already started the Ollama service locally on port `11434`. Run the following Docker command to start OpenGPT locally:

```bash
docker run -d -p 3210:3210 -e OLLAMA_PROXY_URL=http://host.docker.internal:11434/v1 lobehub/lobe-chat
```

Now, you can use OpenGPT to converse with the local LLM.

For more information on using Ollama in OpenGPT, please refer to [Ollama Usage](/docs/usage/providers/ollama).

## Accessing Ollama from Non-Local Locations

When you first initiate Ollama, it is configured to allow access only from the local machine. To enable access from other domains and set up port listening, you will need to adjust the environment variables `OLLAMA_ORIGINS` and `OLLAMA_HOST` accordingly.
```
set OLLAMA_ORIGINS=*
set OLLAMA_HOST=:11434
```
For further guidance on configuration, consult the [Ollama Official Documentation](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server).
